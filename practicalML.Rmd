---
title: "Practical Machine Learning Course Project"
author: "Joshua Paolo Acilo"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

# Introduction

This work is an attempt to predict the manner in which the people did the exercise in the following study.

*Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.*

In the study mentioned above, six participants participated in a dumbell lifting activity five different ways. The five ways, as described in the study, were "exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes."

By processing the data gathered from accelerometers on the belt, forearm, arm, and dumbell of the participants in a machine learning algorithm, can the appropriate activity (class A-E) be predicted?

This report underlines the following task discussed in coursera.
1. How you built your model.
2. How you used cross validation.
3. What you think the expected out of sample error is
4. Why you made the choices you did


---

# Data Pre Processing

Load the following libraries
```{r,warning=F,message=F,echo=F}
library(randomForest)
library(data.table)
library(corrplot)
library(rattle)
library(knitr)
library(caret)
library(dplyr)
library(rpart)
library(rpart.plot)
```

Set a seed for reproducibility of this study
```{r}
set.seed(42)
```

Load the train and test data
```{r}
train <- fread('pml-training.csv')
train$V1 = NULL
train = as.data.frame(train)
test  <- fread('pml-testing.csv')
test$V1 = NULL
test = as.data.frame(test)
```

CHECK
```{r}
c(dim(train),dim(test))
```

Split the training set into 70-30
```{r}
train_part  <- createDataPartition(train$classe, p=0.70, list=FALSE)
train_train <- train[train_part,]
train_valid  <- train[-train_part,]
```

CHECK
```{r}
c(dim(train_train),dim(train_valid))
```

# Feature Selection

Drop the features with near zero variance
```{r}
nzv <- nearZeroVar(train_train)
train_train <- train_train[,-nzv]
train_valid  <- train_valid[,-nzv]
```
*The features with near zero variance are features that doesn't show much variation and therefore is less important in contributing to make a correct prediction as it cannot discern the variability among the targets.*

CHECK
```{r}
c(dim(train_train),dim(train_valid))
```


Drop the features that are mostly NA using 95 threshold
```{r}
all_na <- sapply(train_train, function(x) mean(is.na(x))) > 0.95
train_train <- train_train[,all_na==FALSE]
train_valid  <- train_valid[,all_na==FALSE]
```
*The features with >=95% missing are dropped as these features does not give the model much information in order to make a correct prediction.*

CHECK
```{r}
c(dim(train_train),dim(train_valid))
```

Drop the identifier features
```{r}
train_train <- train_train[,-(1:5)]
train_valid  <- train_valid[,-(1:5)]
```
*The identifier features are directly correlated to each of the targets, and is therefore needed to be dropped.*


CHECK
```{r}
c(dim(train_train),dim(train_valid))
```

Perform correlation analysis
```{r}
corr_mat <- cor(train_train[, -53])
corrplot(corr_mat,order="FPC",method="color",type="lower", 
         tl.cex = 0.5, tl.col = rgb(0, 0, 0))
```
*The colors in the plot above shows the strength of correlation among pairs of features. The darker the color is, the more correlated the pair of features are (red - negatively correlated, blue - positively correlated). Since the strong correlations are just few, further reduction of the number of features is not explored.*


---
---

# Model Building

Set a seed for reproducibility of this study
```{r}
set.seed(42)
```

RF Model Building 1
```{r}
RF <- trainControl(method="cv",number=5,verboseIter=FALSE)
RF <- train(classe~.,data=train_train,method="rf",trControl=RF)
RF$finalModel
```

RF Model Building 2
```{r}
predict_RF <- predict(RF, newdata=train_valid)
confusion_RF <- confusionMatrix(table(predict_RF, train_valid$classe))
confusion_RF
```

RF Model Building 3
```{r}
plot(confusion_RF$table, col = confusion_RF$byClass, 
     main = paste("Random Forest Accuracy = ",round(confusion_RF$overall['Accuracy'], 4)))
```


---

DT Model Building 1
```{r}
DT <- rpart(classe~.,data=train_valid,method="class")
fancyRpartPlot(DT)
```

DT Model Building 2
```{r}
predict_DT <- predict(DT,newdata=train_valid,type="class")
confusion_DT <- confusionMatrix(table(predict_DT,train_valid$classe))
confusion_DT
```

DT Model Building 3
```{r}
plot(confusion_DT$table,col=confusion_DT$byClass, 
     main = paste("Decision Tree Accuracy = ",round(confusion_DT$overall['Accuracy'], 4)))
```


---

GBM Model Building 1
```{r}
GBM <- trainControl(method="repeatedcv",number=5,repeats=1)
GBM  <- train(classe~.,data=train_valid,method="gbm",trControl=GBM,verbose=FALSE)
GBM$finalModel
```


GBM Model Building 2
```{r}
predict_GBM <- predict(GBM,newdata=train_valid)
confusion_GBM <- confusionMatrix(table(predict_GBM,train_valid$classe))
confusion_GBM
```

GBM Model Building 3
```{r}
plot(confusion_GBM$table,col=confusion_GBM$byClass, 
     main = paste("Gradient Boosting Method Accuracy = ",round(confusion_GBM$overall['Accuracy'], 4)))
```


---
---


Predict the test set using the RF
```{r}
predict_TEST <- predict(RF,newdata=test)
predict_TEST[1:20]
```

Predict the test set using the DT
```{r}
predict_TEST <- predict(DT,newdata=test)
predict_TEST[1:20]
```


Predict the test set using the GBM
```{r}
predict_TEST <- predict(GBM,newdata=test)
predict_TEST[1:20]
```


# Conclusion

Based on the metrics presented above, the Random Forest (RF) performed the best in the multiclass classification task. A total of 500 trees were used by the RF with an accuracy of 99.52% in the validation set. Five fold cross validation is performed in the model building in order to get a more accurate measurement of the performance of the trained model. This model is applied in the holdout dataset (testing set) and the first 20 predictions is presented above. 
